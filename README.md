# Read your favourite blog on your Kindle

Blog Downloader is a set of ruby scripts that can turn your favourite blog or podcast websites into RSS feeds.

See examples of `feeds.txt`, `slice-[0-9].xml` & `mp3-urls.txt` generated by blog_downloader [here](https://github.com/goooooouwa/rss-feeds).

## Blogs & Podcasts already supported

- [Coding Horror](https://blog.codinghorror.com)
- [CoolShell](https://coolshell.cn)
- [Maccast Podcast](https://www.maccast.com/category/podcast)
- [Matrix67](http://www.matrix67.com/blog)
- [Mind Hacks](https://mindhacks.cn)
- [White House Press Briefings](https://obamawhitehouse.archives.gov/photos-and-video/video/2017/01/13/11317-white-house-press-briefing?tid=7&x=10&y=11&page=0)

## How to crawl and generate RSS feeds with blog_downloader

Let's use blog Coding Horror as an example.

### 1. Create custom page and post objects along with `config.json` for the website

Coding Horror page and post object:

```ruby
# blogs/coding_horror/coding_horror_page.rb
class CodingHorrorPage < Page
  def initialize(page_url, page_html)
    super(page_url, page_html)
    next_page_url_node = page_html.css(".left .read-next-title a").first
    previous_page_url_node = page_html.css(".right .read-next-title a").first
    @next_page_url = "https://blog.codinghorror.com#{ next_page_url_node.attributes["href"].value }" unless next_page_url_node.nil?
    @previous_page_url = "https://blog.codinghorror.com#{ previous_page_url_node.attributes["href"].value }" unless previous_page_url_node.nil?
    @post_urls = [@page_url]
  end
end

# blogs/coding_horror/coding_horror_posts.rb
class CodingHorrorPost < Post
  def initialize(post_url, post_html)
    super(post_url, post_html)
    @title = post_html.css(".post-title").text
    @published_date = post_html.at("meta[property='article:published_time']")['content']
    @content = post_html.css(".post-content").children
    @author = "Jeff Atwood"
  end
end
```

Config file:

```json
// blogs/coding_horror/config.json
{
    "title": "Coding Horror",
    "description": "programming and human factors",
    "homepage": "https://blog.codinghorror.com",
    "direction": "previous",
    "remote_base_url": "https://raw.githubusercontent.com/goooooouwa/out/master/coding_horror",
    "initial_page": "https://blog.codinghorror.com/building-a-pc-part-ix-downsizing/"
}
```

### 2. Fetch all pages you want to crawl from the website

```bash
ruby ./bin/run.rb page coding_horror
```

### 3. Fetch all posts within the pages you want to crawl

```bash
ruby ./bin/run.rb post coding_horror
```

### 4. Generate RSS feeds from the crawled posts

```bash
ruby ./bin/run.rb render coding_horror   # generate and save RSS feeds as `feeds.txt` & `slice-[0-9].xml` in config["our_dir"]
```

### 5. Publish the generated RSS feeds somewhere to get a public URL

For example, you can publish the RSS feed to a Github repo:

```bash
git add ./out/rss.xml
git commit -m "publish blog feeds"
git push origin master   # publish the RSS file somewhere online to get a public URL
```

### 6. generate the ebook and send it to Kindle

```bash
# in blog2kindle
echo "https://raw.githubusercontent.com/goooooouwa/out/master/out/rss.xml" > config/feeds.txt
python3 src/news2kindle.py "blog title"  # which reads a list of blog RSS feeds, package them as a MOBI file, and then send it to your kindle via kindle mail address and Amazon's whispersync.
```

Now you will have your favourite blog sent to your Kindle, waiting for you to pick up.
