<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title>Microsoft LogParser</title>
<link>https://blog.codinghorror.com/microsoft-logparser/</link>
<content>
                <!--kg-card-begin: markdown--><p>
Ask yourself this question: <b>what if everything could be queried with SQL?</b> <a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=890cd06b-abf8-4c25-91b2-f8d975cf8c07&amp;displaylang=en">Microsoft's LogParser</a> does just that. It lets you slice and dice a variety of log file types using a common SQL-like syntax.  It's an incredibly powerful concept, and the LogParser implementation doesn't disappoint. This architecture diagram from the LogParser documentation explains it better than I could:
</p>
<p>
<img alt="logparser_architecture.gif" border="0" class="at-xid-6a0120a85dcdae970b0128776fb331970c" height="429" src="https://blog.codinghorror.com/content/images/uploads/2005/08/6a0120a85dcdae970b0128776fb331970c-pi.gif" width="414">
</p>
<p>
The excellent <a href="http://securityfocus.com/infocus/1712">forensic IIS log exploration with LogParser</a> article is a good starting point for sample LogParser IIS log queries. Note that I am summarizing just the SQL clauses; I typically output to the console, so the actual, complete commandline would be
</p>
<p>
</p>
<pre>logparser "(sql clause)" -rtp:-1
</pre>
<p>
Top 10 items retrieved:
</p>
<p>
</p>
<pre>SELECT TOP 10 cs-uri-stem as Url, COUNT(cs-uri-stem) AS Hits
FROM ex*.log
GROUP BY cs-uri-stem
ORDER BY Hits DESC
</pre>
<p>
Top 10 slowest items:
</p>
<p>
</p>
<pre>SELECT TOP 10 cs-uri-stem AS Url, MIN(time-taken) as [Min],
AVG(time-taken) AS [Avg], max(time-taken) AS [Max],
count(time-taken) AS Hits
FROM ex*.log
WHERE time-taken &lt; 120000
GROUP BY Url
ORDER BY [Avg] DESC
</pre>
<p>
All Unique Urls retrieved:
</p>
<p>
</p>
<pre>SELECT DISTINCT TO_LOWERCASE(cs-uri-stem) AS Url, Count(*) AS Hits
FROM ex*.log
WHERE sc-status=200
GROUP BY Url
ORDER BY Url
</pre>
<p>
HTTP errors per hour:
</p>
<p>
</p>
<pre>SELECT date, QUANTIZE(time, 3600) AS Hour,
sc-status AS Status, COUNT(*) AS Errors
FROM ex*.log
WHERE (sc-status &gt;= 400)
GROUP BY date, hour, sc-status
HAVING (Errors &gt; 25)
ORDER BY Errors DESC
</pre>
<p>
HTTP errors ordered by Url and Status:
</p>
<p>
</p>
<pre>SELECT cs-uri-stem AS Url, sc-status AS Status, COUNT(*) AS Errors
FROM ex*.log
WHERE (sc-status &gt;= 400)
GROUP BY Url, Status
ORDER BY Errors DESC
</pre>
<p>
Win32 error codes by total and page:
</p>
<p>
</p>
<pre>SELECT cs-uri-stem AS Url,
WIN32_ERROR_DESCRIPTION(sc-win32-status) AS Error, Count(*) AS Total
FROM ex*.log
WHERE (sc-win32-status &gt; 0)
GROUP BY Url, Error
ORDER BY Total DESC
</pre>
<p>
HTTP methods (GET, POST, etc) used per Url:
</p>
<p>
</p>
<pre>SELECT cs-uri-stem AS Url, cs-method AS Method,
Count(*) AS Total
FROM ex*.log
WHERE (sc-status &lt; 400 or sc-status &gt;= 500)
GROUP BY Url, Method
ORDER BY Url, Method
</pre>
<p>
Bytes sent from the server:
</p>
<p>
</p>
<pre>SELECT cs-uri-stem AS Url, Count(*) AS Hits,
AVG(sc-bytes) AS Avg, Max(sc-bytes) AS Max,
Min(sc-bytes) AS Min, Sum(sc-bytes) AS TotalBytes
FROM ex*.log
GROUP BY cs-uri-stem
HAVING (Hits &gt; 100) ORDER BY [Avg] DESC
</pre>
<p>
Bytes sent from the client:
</p>
<p>
</p>
<pre>SELECT cs-uri-stem AS Url, Count(*) AS Hits,
AVG(cs-bytes) AS Avg, Max(cs-bytes) AS Max,
Min(cs-bytes) AS Min, Sum(cs-bytes) AS TotalBytes
FROM ex*.log
GROUP BY Url
HAVING (Hits &gt; 100)
ORDER BY [Avg] DESC
</pre>
<p>
</p>
<p>
There's an <a href="http://www.amazon.com/exec/obidos/ASIN/1932266526/">entire book about LogParser</a>, and Mike Gunderloy even started an <a href="http://www.logparser.com/">unofficial LogParser fansite</a>.
</p>
<p>
Here are a few other articles I found that touch on different aspects of LogParser:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.adopenstatic.com/cs/blogs/ken/archive/2005/05/30/22.aspx">Graphing PING results</a>
</li>
<li>
<a href="http://www.harper.no/valery/PermaLink,guid,b90858aa-16e9-4a7e-a617-f7edce018250.aspx">Generating XML output and Excel 2003 XML pivots</a>
</li>
<li>
<a href="http://www.leastprivilege.com/FunWithLogParser.aspx">Auditing the Event Logs</a>
</li>
<li>
<a href="http://www.furrygoat.com/2005/01/logparser_22.html">Querying an RSS Feed</a>
</li>
<li>
<a href="http://www.microsoft.com/technet/community/columns/profwin/pw0505.mspx">Professor Windows: How LogParser Works</a>
</li>
</ul>
<p>
Although LogParser is 96.44% awesome, there are a few things that I didn't like about it:
</p>
<ol>
<li>I really, really need a standard deviation function. Min, Max, and Avg are nice but totally inadequate for determining how variable something is.
</li>
<li>The graphing output is cool-- but it's also a MS Office dependency. If you try to graph something on a machine without Office installed, you'll get an error.
</li>
<li>The automatic detection of column types in CSV files isn't always reliable. This meant I couldn't graph some numeric values in my PerfMon dumps because LogParser decided they were strings. I couldn't find any way to force a column to be detected as a certain type, either.
</li>
</ol>
<p>
Of course, the idea of SQL being used to query a bunch of stuff isn't exactly a new one; <a href="http://msdn.microsoft.com/library/en-us/wmisdk/wmi/querying_with_wql.asp">Microsoft's WQL</a> (WMI Query Language) is similar but more annoying and less powerful. And you'll get tons of hits if you logically extend this concept to querying HTML, too. Just try <a href="http://www.google.com/search?q=web+query+language">searching Google for Web Query Language</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
            </content>
<pubDate>2005-08-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/microsoft-logparser/</guid>
</item>
</channel>
</rss>
