<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title>Moore's Law in Practical Terms</title>
<link>https://blog.codinghorror.com/moores-law-in-practical-terms/</link>
<content>
                <!--kg-card-begin: markdown--><p>
There are two popular <a href="http://en.wikipedia.org/wiki/Moore's_law#Formulations_of_Moore.27s_Law">formulations of Moore's Law</a>:
</p>
<p>
</p>
<blockquote>
The most popular formulation [of Moore's Law] is <b>the doubling of the number of transistors on integrated circuits every 18 months</b>. At the end of the 1970s, Moore's Law became known as the limit for the number of transistors on the most complex chips. However, it is also common to cite Moore's Law to refer to the rapidly continuing advance in computing power per unit cost, because transistor count is also a rough measure of computer processing power.
</blockquote>
<p>
The number of transistors on a CPU hasn't actually been doubling every 18 months; it's been doubling every 24 months. Here's a graph of <a href="http://en.wikipedia.org/wiki/Transistor_count">the transistor count of each major Intel x86 chip family release</a> from 1971 to 2006:
</p>
<p>
<img alt="Intel x86 transistor counts, 1971-2006" border="0" class="at-xid-6a0120a85dcdae970b0128776feacd970c" height="356" src="https://blog.codinghorror.com/content/images/uploads/2006/12/6a0120a85dcdae970b0128776feacd970c-pi.png" width="459">
</p>
<p>
The dotted line is the predicted transistor count if you doubled the 2,300 transistors from the Intel 4004 chip every two years since 1971.
</p>
<p>
That's why I prefer the second, looser definition of Moore's law: dramatic increases in computing power per unit cost. If you're a stickler for detail, there's an <a href="http://arstechnica.com/articles/paedia/cpu/moore.ars">extensive investigation of Moore's law at Ars Technica</a> you can refer to.
</p>
<p>
But how do we correlate Moore's Law-- the inexorable upward spiral of raw transistor counts-- with performance in practical terms? Personally, I like to look at benchmarks that use "typical" PC applications, such as <a href="http://www.bapco.com/products/sysmark2004/">SysMark 2004</a>. According to page 14 of <a href="http://www.bapco.com/techdocs/SYSmark2004WhitePaper.pdf">this PDF</a>, SysMark 2004 scores are calibrated to a reference system: a Pentium 4 2.0 GHz. The reference system scores 100. <b>Thus, a system which scores 200 in SysMark 2004 will be twice as fast as the reference system.</b>
</p>
<p>
So, what was the first new CPU to <i>double</i> the performance of the SysMark 2004 reference system with a perfect 200? The Pentium 4 "Extreme Edition" 3.2 GHz scores 197 on the SysMark 2004 office benchmark in this <a href="http://www.tomshardware.com/2004/03/18/spring_speed_leap/page25.html">set of Tom's Hardware benchmarks</a>. Let's compare the release dates of these two CPUs:
</p>
<p>
</p>
<table width="300">
<tr>
<td>Pentium 4 2.0 GHz</td>
<td>August 27th, 2001
</td>
</tr>
<tr>
<td>Pentium 4EE 3.2 GHz</td>
<td>November 3rd, 2003
</td>
</tr>
</table>
<p>
It took <b>26 months to double real world performance in SysMark 2004</b>. That tracks almost exactly with the doubling of transistor counts every 24 months.
</p>
<p>
This isn't a perfect comparison, since other parts of the PC get faster at different rates. But it's certainly a good indication that <b>CPU transistor count is fairly reliable indicator of overall performance.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
            </content>
<pubDate>2006-12-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/moores-law-in-practical-terms/</guid>
</item>
</channel>
</rss>
