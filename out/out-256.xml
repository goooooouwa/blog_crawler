<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title>File Compression in the Multi-Core Era</title>
<link>https://blog.codinghorror.com/file-compression-in-the-multi-core-era/</link>
<content>
                <!--kg-card-begin: markdown--><p>
I've been playing around a bit with file compression again, as we generate some very large backup files daily on <a href="http://stackoverflow.com">Stack Overflow</a>.
</p>
<p>
We're using the latest 64-bit version of <a href="http://www.7-zip.org/">7zip</a> (4.64) on our database server. I'm <a href="http://www.codinghorror.com/blog/archives/000942.html">not a big fan of more than dual core on the desktop</a>, but it's a no brainer for servers. The more CPU cores the merrier! This server has two quad-core CPUs, a total of 8 cores, and I was a little disheartened to discover that neither RAR nor 7zip seemed to make much use of more than 2.
</p>
<p>
Still, even if it does only use 2 cores to compress, the 7zip algorithm is amazingly effective, and has <a href="http://www.codinghorror.com/blog/archives/000799.html">evolved over the last few years to be respectably fast</a>. I used to <a href="http://www.codinghorror.com/blog/archives/000798.html">recommend RAR over Zip</a>, but given the increased efficiency of 7zip and the fact that it's free and RAR isn't, it's the logical choice now.
</p>
<p>
Here are some quick tests I performed <b>compressing a single 4.76 GB database backup file</b>. This was run on a server with dual quad-core 2.5 GHz Xeon E5420 CPUs.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="500">
<tr>
<td>7zip</td>
<td>fastest</td>
<td align="right">5 min</td>
<td align="right">14 MB/sec</td>
<td align="right">973 MB</td>
</tr>
<tr>
<td>7zip</td>
<td>fast</td>
<td align="right">7 min</td>
<td align="right">11 MB/sec</td>
<td align="right">926 MB</td>
</tr>
<tr>
<td>7zip</td>
<td>normal</td>
<td align="right">34 min</td>
<td align="right">2.5 MB/sec</td>
<td align="right">752 MB</td>
</tr>
<tr>
<td>7zip</td>
<td>maximum</td>
<td align="right">41 min</td>
<td align="right">2.0 MB/sec</td>
<td align="right">714 MB</td>
</tr>
<tr>
<td>7zip</td>
<td>ultra</td>
<td align="right">48 min</td>
<td align="right">1.7 MB/sec</td>
<td align="right">698 MB</td>
</tr>
</table>
<p>
For those of you who are now wondering, <i>wow, if 7zip does this well on maximum and ultra, imagine how it'd do on ultra-plus</i>, don't count on it. <b>There's a reason most compression programs default to certain settings as "normal".</b> Above these settings, <a href="http://www.codinghorror.com/blog/archives/000313.html">results tend to fall off a cliff</a>; beyond that sweet spot, you tend to get absurdly tiny increases in compression ratio in exchange for huge order of magnitude increases in compression time.
</p>
<p>
Now watch what happens when I switch 7zip to use the <a href="http://en.wikipedia.org/wiki/Bzip2">bzip2 compression algorithm</a>:
</p>
<p>
<img alt="7zip with bzip2 selected" border="0" class="at-xid-6a0120a85dcdae970b0120a86e1a45970b" height="251" src="https://blog.codinghorror.com/content/images/uploads/2009/02/6a0120a85dcdae970b0120a86e1a45970b-pi.png" width="308">
</p>
<p>
We'll compress that same 4.76 GB file, on the same machine:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="500">
<tr>
<td>bzip2</td>
<td>fastest</td>
<td align="right">2 min</td>
<td align="right">36 MB/sec</td>
<td align="right">1092 MB</td>
</tr>
<tr>
<td>bzip2</td>
<td>fast</td>
<td align="right">2.5 min</td>
<td align="right">29 MB/sec</td>
<td align="right">1011 MB</td>
</tr>
<tr>
<td>bzip2</td>
<td>normal</td>
<td align="right">3.5 min</td>
<td align="right">22 MB/sec</td>
<td align="right">989 MB</td>
</tr>
<tr>
<td>bzip2</td>
<td>maximum</td>
<td align="right">7 min</td>
<td align="right">12 MB/sec</td>
<td align="right">987 MB</td>
</tr>
<tr>
<td>bzip2</td>
<td>ultra</td>
<td align="right">21 min</td>
<td align="right">4 MB/sec</td>
<td align="right">986 MB</td>
</tr>
</table>
<p>
Why is bzip2 able to work so much <i>faster</i> than 7zip? Simple:
</p>
<p>
<b>7zip algorithm CPU usage</b>
</p>
<p>
<img alt="7zip multithreaded cpu usage" border="0" class="at-xid-6a0120a85dcdae970b0120a86e1a55970b" height="123" src="https://blog.codinghorror.com/content/images/uploads/2009/02/6a0120a85dcdae970b0120a86e1a55970b-pi.png" width="520">
</p>
<p>
<b>bzip2 algorithm CPU usage</b>
</p>
<p>
<img alt="bzip2-multithreaded-cpu-usage.png" border="0" class="at-xid-6a0120a85dcdae970b0120a86e1a62970b" height="123" src="https://blog.codinghorror.com/content/images/uploads/2009/02/6a0120a85dcdae970b0120a86e1a62970b-pi.png" width="520">
</p>
<p>
<b>Bzip2 uses more than 2 CPU cores to parallelize its work</b>. I'm not sure what the limit is, but the drop-down selector in the 7zip GUI allows up to 16 when the bzip2 algorithm is chosen. I used 8 for the above tests, since that's how many CPU cores we have on the server.
</p>
<p>
Unfortunately, bzip2's increased speed is sort of moot at high compression levels. The difference between normal, maximum, and ultra compression is a meaningless 0.06 percent. It scales beautifully in time, but hardly at all in space. That's a shame, because that's exactly where you'd like to spend the speed increase of paralellization. Eking out a percent of size improvement <a href="http://users.elis.ugent.be/~wheirman/compression/index.php#ranking">could still make sense</a>, depending on the circumstances:
</p>
<p>
</p>
<blockquote>
<i>total time = compression time + n * (compressed file size / network speed + decompression time)</i>
<p>
For instance, if you compress a file to send it over a network once, <i>n</i> equals one and compression time will have a big influence. If you want to post a file to be downloaded many times, <i>n</i> is big so long compression times will weigh less in the final decision. Finally, slow networks will do best with a slow but efficient algorithm, while for fast networks a speedy, possibly less efficient algorithm is needed.
</p>
</blockquote>
<p>
On the other hand, the ability to <b>compress a 5 GB source file to a fifth of its size in two minutes flat</b> is pretty darn impressive. Still, I can't help wondering how fast the 7zip algorithm would be if it was rewritten and parallelized to take advantage of more than 2 CPU cores, too.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
            </content>
<pubDate>2009-02-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/file-compression-in-the-multi-core-era/</guid>
</item>
</channel>
</rss>
