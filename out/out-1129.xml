<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title>Compression and Cliffs</title>
<link>https://blog.codinghorror.com/compression-and-cliffs/</link>
<content>
                <!--kg-card-begin: markdown--><p>
I set up a number of Windows XP SP2 Virtual PC base images today. A WinXP SP2 clean install, after visiting Windows Update, is <b>1.70 gigabytes</b>. Building up a few baseline images like this can chew up a substantial amount of disk space and network bandwidth. So, taking <a href="http://thedotnet.com/blogs/jon_galloway/archive/2005/05/07/343015.aspx">a page from Jon Galloway's book</a>, I decided to see what I'd get if I compressed the virtual hard drive file. My results?
</p>
<p>
</p>
<table>
<tr>
<td>App</td>
<td>Size</td>
<td>Time taken (approx)
</td>
</tr>
<tr>
<td><b>WinZip 9.0 SR-1</b></td>
<td>880 megabytes</td>
<td>3 minutes
</td>
</tr>
<tr>
<td><b>7Zip 4.20</b></td>
<td>739 megabytes</td>
<td>
<span style="color:red;">22 minutes</span>
</td>
</tr>
</table>
<p>
(All apps were used with out of box defaults). I do end up with a file that is 17% smaller, <b>but it takes 7.3 times longer</b>. That sure doesn't seem like a very good deal to me. Now, in fairness to Jon, his only goal was to squeeze a largish 10gb VHD image into a single 4.7 gigabyte DVD-R; compression time wasn't a criteria.
</p>
<p>
Although this is my first exposure to 7zip, I've run these kinds of comparions before with ZIP and RAR and reached the same conclusion. Although there are certainly different algorithmic efficiencies, no matter what compression algorithm you choose-- <b>beyond a certain optimal compression level, performance falls off a cliff.</b> After you hit that point, you'll spend obscene amounts of time for rapidly diminishing benefits. Nowhere is this better illustrated than in Wim Heirman's <a href="http://www.elis.ugent.be/~wheirman/compression/">Practical Compressor Test</a> results:
</p>
<p>
<img alt="GIMP source compression results, compression ratio vs. time" border="0" class="at-xid-6a0120a85dcdae970b0128776fb70b970c" height="320" src="https://blog.codinghorror.com/content/images/uploads/2005/06/6a0120a85dcdae970b0128776fb70b970c-pi.png" width="465">
</p>
<p>
Note that the scale on the bottom of the graph is <i>logarithmic</i>. This is the only comparison I could find that properly expresses compression as the zero-sum game it really is: you can either have efficiency, or you can have speed. That's why, except for the truly obsolete algorithms, you see the "diagonal line" effect on this graph: better compression algorithms always take longer. Sometimes a <i>lot</i> longer. If you're holding out for Hyper Turbo Extreme Compression X algorithm, you may be waiting a while. Consider RAR, which offers the best blend of compression and speed currently available:
</p>
<p>
</p>
<table>
<tr>
<td>Level</td>
<td>Time (secs)</td>
<td>Compression ratio</td>
<td>Time factor</td>
<td>Gain
</td>
</tr>
<tr>
<td>-m1</td>
<td>5.7</td>
<td>22.1%</td>
<td>1x</td>
<td>-
</td>
</tr>
<tr>
<td>-m2</td>
<td>28.3</td>
<td>14.5%</td>
<td><span style="color:red;">5x longer</span></td>
<td>
<span style="color:red;">7.6% smaller</span>
</td>
</tr>
<tr>
<td>-m3</td>
<td>40.2</td>
<td>13.4%</td>
<td><span style="color:red;">7x longer</span></td>
<td>
<span style="color:red;">8.7% smaller</span>
</td>
</tr>
<tr>
<td>-m4</td>
<td>40.2</td>
<td>13.1%</td>
<td><span style="color:red;">7x longer</span></td>
<td>
<span style="color:red;">9.0% smaller</span>
</td>
</tr>
<tr>
<td>-m5</td>
<td>46.7</td>
<td>12.5%</td>
<td><span style="color:red;">8x longer</span></td>
<td>
<span style="color:red;">9.6% smaller</span>
</td>
</tr>
</table>
<p>
<b>When it takes 5 times longer for barely 8% more compression, you've fallen off a cliff.</b> But it still might be worth the extreme cost, depending on your goals. For most usages, it boils down to these three questions:
</p>
<ol>
<li>How often will your data be compressed?
</li>
<li>How many times will it be transferred?
</li>
<li>How fast is the network?
</li>
</ol>
<p>
Decompression time in this scenario is usually a tiny, relatively constant fraction of the compression time, so it's not a factor. Wim provides <a href="http://www.elis.ugent.be/~wheirman/compression/index.php#ranking">a helpful calculator</a> to assist in making this decision:
</p>
<p>
</p>
<blockquote>
<i>
total time = compression time + n * (compressed file size / network speed + decompression time)
</i><p>
For instance, if you compress a file to send it over a network once, n equals one and compression time will have a big influence. If you want to post a file to be downloaded many times, n is big so long compression times will weigh less in the final decision. Finally, slow networks will do best with a slow but efficient algorithm, while for fast networks a speedy, possibly less efficient algorithm is needed.
</p>
</blockquote>
<p>
Of course, there are still a few variables Wim's page hasn't considered. Most notably, he only compresses a single file (the GIMP source TAR file), which has two consequences:
</p>
<ol>
<li>Filetype specific compression can perform far better than the generic compression considered here. Compression tailored to file contents (eg, <a href="http://www.firstpr.com.au/audiocomp/lossless/">lossless audio compression</a>) is generally a huge win.
</li>
<li>When compressing groups of files, programs that can do "solid" archiving will always outperform those that can't. Solid archiving means that the files are compressed as one giant binary blob and not as a bunch of individual files. This provides a much higher level of overall compression due to (generally) repeated data between files.
</li>
</ol>
<p>
No one set of benchmarks offers a complete picture. <a href="http://www.maximumcompression.com/benchmarks/benchmarks.php">Most other compression benchmark pages</a> tend to focus on absolute compression ratios to the detriment of all other variables, which is a little crazy once you've fallen off the cliff. On Wim's page, the slowest three times are 198 (7zip), 47 (rar), and 43 (bzip2) seconds respectively. Some of the <a href="http://www.cs.fit.edu/~mmahoney/compression/">more extreme space-optimized compression algorithms</a> can take <b>several hours to compress the same file!</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
            </content>
<pubDate>2005-06-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/compression-and-cliffs/</guid>
</item>
</channel>
</rss>
